{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Llama\n",
    "Trying out different local LLMs and creating a library to use them easily.\n",
    "The different methods we'll try are:\n",
    "1. Llama-cpp\n",
    "2. Ollama\n",
    "3. Langchain\n",
    "4. LlamaIndex\n",
    "5. Huggingface\n",
    "\n",
    "We are also only looking now at models that can utilize the Mac's M1 chip, isnce we're developing on it. This can potentially limit the models we can use, but we will see. In production environemnts it might be better to separate the model API and the application, since the model will need at GPU to run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "Ollama is the easiest to get up and running, when it comes to local models. \n",
    "[Link](https://github.com/ollama/ollama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
